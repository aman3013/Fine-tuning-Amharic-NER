{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDTKXsPwzDJrQhFSERH9X2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aman3013/Fine-tuning-Amharic-NER/blob/Task-4/Model_Comparison_%26_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "id": "zDOLCSOMSPdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "jSz1WSoUV49V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "QLtnNtg2Slej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "id": "l_D1ukP_TAA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets seqeval scikit-learn\n"
      ],
      "metadata": {
        "id": "sZurmDgJVLAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer, DistilBertTokenizer, BertTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "import evaluate  # Updated import statement\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Oah61CXkNZCS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdzxN31LQj3r",
        "outputId": "9cb07e36-5f58-46c4-ce6a-ffea073dd5f6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Zst6m4MiX6K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load the dataset from the specified path\n",
        "data_path = '/content/drive/My Drive/merged_file.conll'  # Update this path to the correct location\n",
        "\n",
        "try:\n",
        "    # Load the data and ensure it's a DataFrame\n",
        "    data = pd.read_csv(data_path, sep=\"\\t\", header=None, names=[\"word\", \"label\"])\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        print(\"Data loaded successfully.\")\n",
        "    else:\n",
        "        raise ValueError(\"Loaded data is not a DataFrame.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found at path: {data_path}\")\n",
        "    data = None\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    data = None\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y81PCHU2QtJW",
        "outputId": "861f991c-e967-455e-beab-e4df83c6d03f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully.\n",
            "   Word Label\n",
            "0   ይህን     O\n",
            "1   መፍጫ     O\n",
            "2  ከሁሉም     O\n",
            "3  የተሻለ     O\n",
            "4    ሆኖ     O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for None values in the 'label' column\n",
        "none_count = data['label'].isnull().sum()\n",
        "print(f\"Number of None values in labels: {none_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtue3i8ObeFQ",
        "outputId": "899202f6-cae9-43d0-8c5e-dc0c57aa7ab2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of None values in labels: 130150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with None labels\n",
        "data = data[data['label'].notnull()]\n"
      ],
      "metadata": {
        "id": "DOgJPgPNbgXi"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace None with a default label (e.g., \"O\")\n",
        "data['label'] = data['label'].fillna('O')\n"
      ],
      "metadata": {
        "id": "H7gfGT4kbkSB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJWpwXipbqEa",
        "outputId": "a482b979-7412-4ac7-92fa-999730d0bb8d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Word Label\n",
            "0   ይህን     O\n",
            "1   መፍጫ     O\n",
            "2  ከሁሉም     O\n",
            "3  የተሻለ     O\n",
            "4    ሆኖ     O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for None values in the 'label' column\n",
        "none_count = data['label'].isnull().sum()\n",
        "print(f\"Number of None values in labels: {none_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNUoofq1bu8T",
        "outputId": "4bdcb1a9-42d8-4154-e4ee-eb5cfc732ad2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of None values in labels: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Proceed only if data is successfully loaded\n",
        "if data is not None:\n",
        "    # Step 5: Convert to Hugging Face Dataset\n",
        "    dataset = Dataset.from_pandas(data)\n",
        "    print(\"Dataset converted to Hugging Face format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iLt6o_TYz1D",
        "outputId": "fee3684f-5e2e-4225-842e-f245e0ce1d3d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset converted to Hugging Face format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Step 6: Define label mapping\n",
        "    unique_labels = list(set(data['label'].values))\n",
        "    label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    id2label = {idx: label for label, idx in label2id.items()}"
      ],
      "metadata": {
        "id": "PwDld9jnYoPG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Step 7: Tokenization function\n",
        "def tokenize_and_align_labels(examples):\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        tokenized_inputs = tokenizer(examples['word'], truncation=True, is_split_into_words=True)\n",
        "\n",
        "        # Align labels with tokenized inputs\n",
        "        labels = []\n",
        "        for i, label in enumerate(examples['label']):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            label_ids = [-100 if idx is None else label2id[label] for idx in word_ids]\n",
        "            labels.append(label_ids)\n",
        "\n",
        "        tokenized_inputs['labels'] = labels\n",
        "        return tokenized_inputs"
      ],
      "metadata": {
        "id": "YPoKfPxCY8J0"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        " # Step 8: Tokenize the dataset\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "APv5O_9oYTq1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "2hbplFjxcBnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Split the dataset into training and validation sets\n",
        "    train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "    train_dataset = train_test_split['train']\n",
        "    val_dataset = train_test_split['test']"
      ],
      "metadata": {
        "id": "spCI6VXIYWJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Step 10: Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',          # output directory\n",
        "        evaluation_strategy=\"epoch\",     # evaluation strategy\n",
        "        learning_rate=2e-5,              # learning rate\n",
        "        per_device_train_batch_size=16,  # batch size for training\n",
        "        per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "        num_train_epochs=3,              # total number of training epochs\n",
        "        weight_decay=0.01,               # strength of weight decay\n",
        "    )"
      ],
      "metadata": {
        "id": "bB4uWNsdYH7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply tokenization\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
      ],
      "metadata": {
        "id": "_UYJ77jEUKZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Define models to compare\n",
        "models_to_compare = [\n",
        "    \"bert-base-multilingual-cased\",  # mBERT\n",
        "    \"xlm-roberta-base\",               # XLM-Roberta\n",
        "    \"distilbert-base-multilingual-cased\",  # DistilBERT\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for model_name in models_to_compare:\n",
        "    # Load the model\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(unique_labels), label2id=label2id)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{model_name}\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,  # You may want to split this into train/val sets\n",
        "        eval_dataset=tokenized_dataset,\n",
        "        compute_metrics=lambda p: {\n",
        "            'precision': precision_score(p.predictions, p.label_ids, average='weighted', zero_division=0),\n",
        "            'recall': recall_score(p.predictions, p.label_ids, average='weighted', zero_division=0),\n",
        "            'f1': f1_score(p.predictions, p.label_ids, average='weighted', zero_division=0),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    # Store the evaluation results for comparison\n",
        "    results[model_name] = eval_results\n"
      ],
      "metadata": {
        "id": "UrDWCOwfQ2St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display comparison results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Evaluation Results: {metrics}\")\n"
      ],
      "metadata": {
        "id": "iUaJW6wrQ4z1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}